# Compression Algorithms

This directory contains detailed documentation for each compression algorithm implemented in the project.

## Entropy-Based Algorithms

Entropy-based algorithms use statistical analysis and information theory to encode data more efficiently based on symbol frequencies.

- **[Huffman Coding](huffman.md)** âœ… - Variable-length prefix coding based on symbol frequencies
- **Shannon-Fano** ðŸš§ - Early entropy coding, predecessor to Huffman
- **Arithmetic Coding** ðŸ“‹ - Encodes entire message as a single fractional number
- **Range Encoding** ðŸ“‹ - Simplified variant of arithmetic coding

## Dictionary-Based Algorithms

Dictionary-based algorithms replace repeated sequences with references to a dictionary of previously seen patterns.

- **LZ77** ðŸ“‹ - Sliding window dictionary compression
- **LZ78** ðŸ“‹ - Dynamic dictionary building
- **LZW** ðŸ“‹ - Lempel-Ziv-Welch, used in GIF and TIFF
- **LZMA** ðŸ“‹ - High compression ratio dictionary algorithm

## Transform-Based Algorithms

Transform-based algorithms apply reversible transformations to make data more compressible for subsequent encoding.

- **BWT (Burrows-Wheeler Transform)** ðŸ“‹ - Reversible block-sorting transform
- **MTF (Move-to-Front)** ðŸ“‹ - Improves locality of reference
- **Delta Encoding** ðŸ“‹ - Stores differences between consecutive values

## Hybrid Algorithms

Hybrid algorithms combine multiple compression techniques for optimal results.

- **DEFLATE** ðŸ“‹ - Combines LZ77 and Huffman (used in ZIP, gzip)
- **BZIP2** ðŸ“‹ - Combines BWT, RLE, and Huffman
- **LZMA2** ðŸ“‹ - Enhanced LZMA with multithreading support

## Context-Based Algorithms

Context-based algorithms use surrounding data to predict and encode information more efficiently.

- **PPM (Prediction by Partial Matching)** ðŸ“‹ - Context modeling for text
- **Context Mixing** ðŸ“‹ - Combines multiple context models

## Neural Network-Based Algorithms

Experimental algorithms using machine learning for compression.

- **N-gram Arithmetic** ðŸ“‹ - Statistical language models with arithmetic coding
- **LSTM Arithmetic** ðŸ“‹ - LSTM networks for prediction
- **Transformer Compression** ðŸ“‹ - Transformer-based compression
- **VAE (Variational Autoencoder)** ðŸ“‹ - Neural network compression

---

## Legend

- âœ… Fully documented
- ðŸš§ Work in progress
- ðŸ“‹ Planned

## Contributing Documentation

When documenting a new algorithm, please include:

1. **Overview** - Brief description and use cases
2. **Theory** - Mathematical/theoretical foundation
3. **Algorithm** - Step-by-step explanation
4. **Implementation** - Kotlin code walkthrough
5. **Performance** - Time/space complexity
6. **Examples** - Usage examples with sample data
7. **References** - Academic papers, books, articles

See [huffman.md](huffman.md) as a template.
